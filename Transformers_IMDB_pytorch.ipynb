{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "test_iter = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, \"I LOVED this movie! I am biased seeing as I am a huge Disney fan, but I really enjoyed myself. The action takes off running in the beginning of the film and just keeps going! This is a bit of a departure for Disney, they don't spend quite as much time on character development (my husband pointed this out)and there are no musical numbers. It is strictly action adventure. I thoroughly enjoyed it and recommend it to anyone who loves Disney, be they young or old.\")\n",
      "(1, 'This was an abysmal show. In short it was about this kid called Doug who guilt-tripped a lot. Seriously he could feel guilty over killing a fly then feeling guilty over feeling guilty for killing the fly and so forth. The animation was grating and unpleasant and the jokes cheap. <br /><br />It aired here in Sweden as a part of the \"Disney time\" show and i remember liking it some what but then i turned 13.<br /><br />I never got why some of the characters were green and purple too. What was up with that? <br /><br />Truly a horrible show. Appareantly it spawned a movie which i\\'ve never seen but i don\\'t have any great expectations for that one either.')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(6)\n",
    "\n",
    "# train_iter를 리스트 타입으로 변경\n",
    "train_lists = list(train_iter)\n",
    "test_lists = list(test_iter)\n",
    "\n",
    "# 각기 1000개씩 랜덤 샘플링\n",
    "train_lists_small = random.sample(train_lists, 1000)\n",
    "test_lists_small = random.sample(test_lists, 1000)\n",
    "\n",
    "# 각 변수에 담긴 인덱스 0에 해당하는 원소, 즉 첫 번째 원소 출력\n",
    "print(train_lists_small[0])\n",
    "print(test_lists_small[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "train_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I LOVED this movie! I am biased seeing as I am a huge Disney fan, but I really enjoyed myself. The action takes off running in the beginning of the film and just keeps going! This is a bit of a departure for Disney, they don't spend quite as much time on character development (my husband pointed this out)and there are no musical numbers. It is strictly action adventure. I thoroughly enjoyed it and recommend it to anyone who loves Disney, be they young or old.\n",
      "1\n",
      "This was an abysmal show. In short it was about this kid called Doug who guilt-tripped a lot. Seriously he could feel guilty over killing a fly then feeling guilty over feeling guilty for killing the fly and so forth. The animation was grating and unpleasant and the jokes cheap. <br /><br />It aired here in Sweden as a part of the \"Disney time\" show and i remember liking it some what but then i turned 13.<br /><br />I never got why some of the characters were green and purple too. What was up with that? <br /><br />Truly a horrible show. Appareantly it spawned a movie which i've never seen but i don't have any great expectations for that one either.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for label, text in train_lists_small:\n",
    "  # IMDB 데이터의 기준 레이블 2를 1 변경, 기본 레이블 1을 0으로 변경\n",
    "  train_labels.append(1 if label==2 else 0)\n",
    "  train_texts.append(text)\n",
    "\n",
    "# test도 동일시\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "\n",
    "for label, text in test_lists_small:\n",
    "  # IMDB 데이터의 기준 레이블 2를 1 변경, 기본 레이블 1을 0으로 변경\n",
    "  test_labels.append(1 if label==2 else 0)\n",
    "  test_texts.append(text)\n",
    "  \n",
    "\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])\n",
    "print(test_texts[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "800\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=3)\n",
    "\n",
    "print(len(train_texts))\n",
    "print(len(train_labels))\n",
    "print(len(val_texts))\n",
    "print(len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seolhun/miniconda3/envs/pet/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# distilbert-base-uncased 모델에서 토크나이저 불러오기\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 4937, 11350, 2038, 2048]\n",
      "[CLS] cat soup has two\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 실행\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True) # truncation은 모델의 디폴트 max_length를 넘는 입력 부분은 더 이상 받지 않고 절단\n",
    "\n",
    "# 0번째 입력문(텍스트)의 5번째 토큰까지의 input_ids 출력\n",
    "print(train_encodings[\"input_ids\"][0][:5])\n",
    "\n",
    "# 위의 결과를 디코딩하여 출력\n",
    "print(tokenizer.decode(train_encodings[\"input_ids\"][0][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "  \n",
    "  # 생성자 __init__()\n",
    "  # 자신을 가리키는 매개변수 self 포함\n",
    "  # 변수를 저장하기 위해 self.변수명을 사용\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings = encodings \n",
    "    self.labels = labels\n",
    "  \n",
    "  # 자신을 가리키는 매개변수 self 포함\n",
    "  def __getitem__(self, idx):\n",
    "    # self.encoding에 담긴 키(key)와 키값(value)을 items()로 추출\n",
    "    # 이 값을 key와 val 변수에 담아 새로운 키(key)와\n",
    "    # 키값을 갖는 딕셔너리 생성\n",
    "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "  \n",
    "  # 자신을 가리키는 매개변수 self 포함\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "  \n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(model, tokenizer):\n",
    "    input_tokens = tokenizer([\"I feel fantastic\", \"My life is going something wrong\", \"I have not figured out what the chosen title has to do with the movie\"], truncation=True, padding=True)\n",
    "\n",
    "    outputs = model(torch.tensor(input_tokens['input_ids']).to('mps'))\n",
    "    label_dict = {1: 'positive', 0: 'negative'}\n",
    "\n",
    "    return [label_dict[i] for i in torch.argmax(outputs['logits'], axis=1).cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'positive', 'positive']\n",
      "epoch:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seolhun/miniconda3/envs/pet/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n",
      "epoch:2\n",
      "epoch:3\n",
      "epoch:4\n",
      "epoch:5\n",
      "epoch:6\n",
      "epoch:7\n",
      "['positive', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to('mps')\n",
    "\n",
    "print(test_inference(model, tokenizer))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(8):\n",
    "    print(f'epoch:{epoch}')\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to('mps')\n",
    "        attention_mask = batch['attention_mask'].to('mps')\n",
    "        labels = batch['labels'].to('mps')\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        losses.append(loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "model.eval()\n",
    "\n",
    "print(test_inference(model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "l = []\n",
    "\n",
    "for test_text in test_texts:\n",
    "\n",
    "    input_tokens = tokenizer([test_text], truncation=True, padding=True)\n",
    "\n",
    "    outputs = model(torch.tensor(input_tokens['input_ids']).to('mps'))\n",
    "\n",
    "    l.append(torch.argmax(outputs['logits'], axis=1).item())\n",
    "\n",
    "correct_cnt= 0\n",
    "\n",
    "for pred, ans in zip(l, test_labels):\n",
    "    if pred == ans:\n",
    "        correct_cnt += 1\n",
    "print(correct_cnt/len(test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
